from scipy.sparse import lil_matrix
from sklearn.feature_extraction.text import TfidfTransformer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
import sys
import pickle
import random
from var import *


def file_to_word_set(filename):
    """ Converts a file with a word per line to a Python set """
    words = []
    with open(filename, "r") as f:
        for line in f:
            words.append(line.strip())
    return set(words)


def write_status(i, total):
    """ Writes status of a process to console """
    sys.stdout.write("\r")
    sys.stdout.write("Processing %d/%d" % (i, total))
    sys.stdout.flush()


def save_results_to_csv(results, csv_file):
    """ Save list of type [(tweet_id, prediction, tweet)] to csv """
    with open(csv_file, 'w') as csv:
        csv.write("tweet_id,prediction,tweet\n")
        for tweet_id, pred, tweet in results:
            csv.write(tweet_id)
            csv.write(",")
            csv.write(str(pred))
            csv.write(",")
            csv.write(tweet)
            # csv.write("\n")
        csv.close()
        print(f"\nSaved to {csv_file}")


def top_n_words_rank(pkl_file_name, no_of_words, shift=0):
    """
    Returns a dictionary of form {word:rank} of top N words from a pickle
    file which has a nltk FreqDist object generated by stats.py

    Args:
        pkl_file_name (str): Name of pickle file
        no_of_words (int): The number of words to get
        shift: amount to shift the rank from 0.
    Returns:
        dict: Of form {word:rank}
    """
    with open(pkl_file_name, "rb") as pkl_file:
        freq_dist = pickle.load(pkl_file)
        pkl_file.close()
    most_common = freq_dist.most_common(no_of_words)
    words = {p[0]: i + shift for i, p in enumerate(most_common)}
    return words


# def top_n_bi_grams(pkl_file_name, no_of_bi_grams, shift=0):
#     """
#     Returns a dictionary of form {bi_gram:rank} of top N bigrams from a pickle
#     file which has a Counter object generated by stats.py
#
#     Args:
#         pkl_file_name (str): Name of pickle file
#         no_of_bi_grams (int): The number of bigrams to get
#         shift: amount to shift the rank from 0.
#     Returns:
#         dict: Of form {bi_gram:rank}
#     """
#     with open(pkl_file_name, "rb") as pkl_file:
#         freq_dist = pickle.load(pkl_file)
#         pkl_file.close()
#     most_common = freq_dist.most_common(no_of_bi_grams)
#     bi_grams_dict = {p[0]: i + shift for i, p in enumerate(most_common)}
#     return bi_grams_dict


def split_data(tweets, validation_split=0.1):
    """Split the data into training and validation sets

    Args:
        tweets (list): list of tuples
        validation_split (float): validation split %

    Returns:
        (list, list): training-set, validation-set
    """
    index = int((1 - validation_split) * len(tweets))
    random.shuffle(tweets)
    return tweets[:index], tweets[index:]


def get_feature_vector(tweet):
    uni_feature_vector = []
    bi_feature_vector = []
    words = tweet.split()
    for i in range(len(words) - 1):
        word = words[i]
        next_word = words[i + 1]
        if uni_grams.get(word):
            uni_feature_vector.append(word)
        if use_bi_grams:
            if bi_grams.get((word, next_word)):
                bi_feature_vector.append((word, next_word))
    if len(words) >= 1:
        if uni_grams.get(words[-1]):
            uni_feature_vector.append(words[-1])
    return uni_feature_vector, bi_feature_vector


def extract_features(tweets, batch_size=500, test_file=True, feat_type="presence"):
    num_batches = int(np.ceil(len(tweets) / float(batch_size)))
    for i in range(num_batches):
        if test_file:
            write_status(i + 1, num_batches)
        batch = tweets[i * batch_size: (i + 1) * batch_size]
        features = lil_matrix((batch_size, vocab_size))
        labels = np.zeros(batch_size)
        for j, tweet in enumerate(batch):
            if test_file:
                tweet_words = tweet[1][0]
                tweet_bigrams = tweet[1][1]
            else:
                tweet_words = tweet[2][0]
                tweet_bigrams = tweet[2][1]
                labels[j] = tweet[1]
            if feat_type == "presence":
                tweet_words = set(tweet_words)
                tweet_bigrams = set(tweet_bigrams)
            for word in tweet_words:
                idx = uni_grams.get(word)
                if idx:
                    features[j, idx] += 1
            if use_bi_grams:
                for bi_gram in tweet_bigrams:
                    idx = bi_grams.get(bi_gram)
                    if idx:
                        features[j, uni_gram_size + idx] += 1
        yield features, labels


def apply_tf_idf(x):
    """
    Fits X for TF-IDF vectorization and returns the transformer.
    """
    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)
    transformer.fit(x)
    return transformer


def process_tweets(csv_file, test_file=True):
    """Returns a list of tuples of type (tweet_id, feature_vector)
            or (tweet_id, sentiment, feature_vector)

    Args:
        csv_file (str): Name of processed csv file generated by preprocess.py
        test_file (bool, optional): If processing test file

    Returns:
        list: Of tuples
    """
    tweets = []
    print("Generating feature vectors")
    with open(csv_file, "r") as csv:
        lines = csv.readlines()
        total_tweets = len(lines)
        for i, line in enumerate(lines):
            if test_file:
                tweet_id, tweet = line.split(",")
            else:
                tweet_id, sentiment, tweet = line.split(",")
            feature_vector = get_feature_vector(tweet)
            if test_file:
                tweets.append((tweet_id, feature_vector, tweet))
            else:
                tweets.append((tweet_id, int(sentiment), feature_vector))
            write_status(i + 1, total_tweets)
    print('\n')
    return tweets


def train_model(training_tweets, classifier):
    print("Extracting features & training model")
    batch_size = len(training_tweets)
    for training_set_X, training_set_y in extract_features(training_tweets, test_file=False, feat_type=feat_type,
                                                           batch_size=batch_size):
        if feat_type == "frequency":
            tfidf = apply_tf_idf(training_set_X)
            training_set_X = tfidf.transform(training_set_X)
        classifier.fit(training_set_X, training_set_y)
    print("\nTraining complete")


def validate_model(val_tweets, classifier):
    print("\nValidating model")
    correct, total = 0, len(val_tweets)
    batch_size = len(val_tweets)
    for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=feat_type,
                                                 batch_size=batch_size):
        if feat_type == "frequency":
            tfidf = apply_tf_idf(val_set_X)
            val_set_X = tfidf.transform(val_set_X)
        prediction = classifier.predict(val_set_X)
        correct += np.sum(prediction == val_set_y)
    print("\nAccuracy: %d/%d = %.4f %%" % (correct, total, correct * 100. / total))


def open_model(file_name):
    try:
        with open(file_name, "rb") as file:
            classifier = pickle.load(file)
            file.close()
            return classifier
    except FileNotFoundError:
        print("First train the model.")
        exit()


def predict_using_model(test_tweets, classifier):
    print("Predicting")
    predictions = np.array([])
    for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=feat_type):
        if feat_type == "frequency":
            tfidf = apply_tf_idf(test_set_X)
            test_set_X = tfidf.transform(test_set_X)
        prediction = classifier.predict(test_set_X)
        predictions = np.concatenate((predictions, prediction))
    predictions = [(test_tweets[j][0], int(predictions[j]), test_tweets[j][2]) for j in range(len(test_tweets))]
    return predictions


def train_validate_save_model(model, nb_epochs, file_name):
    tweets = process_tweets(train_processed_file, test_file=False)
    train_tweets, val_tweets = split_data(tweets)
    print("Extracting features & training model")
    batch_size = 500
    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))
    best_val_acc = 0.0
    for j in range(nb_epochs):
        i = 1
        for training_set_X, training_set_y in extract_features(train_tweets, feat_type=feat_type,
                                                               batch_size=batch_size, test_file=False):
            model.train_on_batch(training_set_X, training_set_y)
            sys.stdout.write("\rIteration %d/%d" % (i, n_train_batches))
            sys.stdout.flush()
            i += 1
        val_acc = validate_neural_model(val_tweets, model)
        print("\nEpoch: %d, Accuracy: %.4f %%\n" % (j + 1, val_acc))
        random.shuffle(train_tweets)
        if val_acc > best_val_acc:
            print("Accuracy improved from %.4f %% to %.4f %%, saving model" % (best_val_acc, val_acc))
            best_val_acc = val_acc
            model.save(file_name)
            print(f"Saved to {file_name}\n")
    del train_tweets
    del model


def validate_neural_model(val_tweets, classifier):
    correct, total = 0, len(val_tweets)
    for val_set_X, val_set_y in extract_features(val_tweets, batch_size=total, feat_type=feat_type, test_file=False):
        current_prediction = classifier.predict_on_batch(val_set_X)
        current_prediction = np.round(current_prediction)
        correct += np.sum(current_prediction == val_set_y[:, None])
    return float(correct) * 100 / total


def predicting_neural_model(test_tweets, classifier):
    print("Predicting")
    predictions = np.array([])
    for test_set_X, _ in extract_features(test_tweets, feat_type=feat_type, test_file=True):
        prediction = np.round(classifier.predict_on_batch(test_set_X).flatten())
        predictions = np.concatenate((predictions, prediction))
    predictions = [(test_tweets[j][0], int(predictions[j]), test_tweets[j][2]) for j in range(len(test_tweets))]
    return predictions


def is_train():
    train_prediction = input("Enter your choice: \n 1) Train \n 2) Prediction\n")
    if train_prediction == "1":
        return True
    elif train_prediction == "2":
        return False
    else:
        print("Wrong option.")
        exit()


def get_neural_feature_vector(vocab, tweet):
    words = tweet.split()
    feature_vector = []
    for _ in range(len(words) - 1):
        word = words[_]
        if vocab.get(word) is not None:
            feature_vector.append(vocab.get(word))
    if len(words) >= 1:
        if vocab.get(words[-1]) is not None:
            feature_vector.append(vocab.get(words[-1]))
    return feature_vector


def get_glove_vectors(vocabs):
    """
    Extracts glove vectors from seed file only for words present in vocab.
    Args:
        vocabs: dict {word: rank}

    Returns:
        glove_vectors_dict: dict {word: glove vector}
    """
    print("Looking for GLOVE vectors")
    glove_vectors_dict = {}
    found = 0
    with open(glove_file_path, "r", encoding="utf-8") as glove_file:
        for _, line in enumerate(glove_file):
            sys.stdout.write(f"\rProcessing {_}")
            sys.stdout.flush()
            tokens = line.split()
            current_word = tokens[0]
            if vocabs.get(current_word):
                vector = [float(e) for e in tokens[1:]]
                glove_vectors_dict[current_word] = np.array(vector)
                found += 1
    print("\nFound %d words in GLOVE" % found)
    return glove_vectors_dict


def process_tweets_neural(csv_file, test_file: bool = True):
    """
    Generates a feature vector for each tweet where each word is
    represented by integer index based on rank in vocabulary.
    Args:
        csv_file: file containing tweets
        test_file: whether training the model or predicting using model

    Returns:

    """
    tweets_feature_vector = []
    label = []
    print("Generating feature vectors")
    with open(csv_file, "r") as csv:
        lines = csv.readlines()
        total = len(lines)
        for _, line in enumerate(lines):
            if test_file:
                tweet_id, tweet = line.split(",")
            else:
                tweet_id, sentiment, tweet = line.split(",")
            current_feature_vector = get_neural_feature_vector(uni_grams, tweet)
            if test_file:
                tweets_feature_vector.append((tweet_id, current_feature_vector, tweet))
            else:
                tweets_feature_vector.append(current_feature_vector)
                label.append(int(sentiment))
            write_status(_ + 1, total)
    print("\n")
    return tweets_feature_vector, np.array(label)


def predicting_cnn_model(classifier, max_length, file_name):
    output_folder = "../results/"
    output_file = output_folder + file_name
    test_tweets0, _ = process_tweets_neural(test_processed_file, test_file=True)
    test_tweets = []
    for i in test_tweets0:
        feature_vector = i[1]
        test_tweets.append(feature_vector)
    test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding="post")
    predictions = classifier.predict(test_tweets, batch_size=128, verbose=1)
    predictions = np.round(predictions[:, 0]).astype(int)
    predictions = predictions.tolist()
    predictions = [(test_tweets0[j][0], int(predictions[j]), test_tweets0[j][2]) for j in range(len(test_tweets0))]
    save_results_to_csv(predictions, output_file)


def initialize_glove(dim, max_length):
    tweets, labels = process_tweets_neural(train_processed_file, test_file=False)
    glove_vectors = get_glove_vectors(uni_grams)
    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01
    for word, i in uni_grams.items():
        glove_vector = glove_vectors.get(word)
        if glove_vector is not None:
            embedding_matrix[i] = glove_vector
    tweets = pad_sequences(tweets, maxlen=max_length, padding="post")
    shuffled_indices = np.random.permutation(tweets.shape[0])
    tweets = tweets[shuffled_indices]
    labels = labels[shuffled_indices]
    return tweets, labels, embedding_matrix


# TODO: If possible place these two function call only in file where it is required.
uni_grams = top_n_words_rank(freq_dist_file, uni_gram_size)
if use_bi_grams:
    bi_grams = top_n_words_rank(bi_freq_dist_file, bi_gram_size)
